{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "<img src=\"images/RL.png\" width=400>\n",
    "\n",
    "\n",
    "<b>Agent:</b> A cute robot. The goal of the agent is to pick the best policy that will maximize the total rewards received from the environment.\n",
    "<img src=\"images/walle.png\" width=100>\n",
    "\n",
    "<b>Environment:</b> The world the Agent interacts with.\n",
    "\n",
    "<b>State:</b>  $S(t)$ The current configuration of the environment.\n",
    "\n",
    "<b>Action:</b> $A(t)$ Things agent can do that will affect its state.\n",
    "\n",
    "<b>Reward:</b> $R(t)$ Result of the action. Can be negative or positive. Represents how good the action was. \n",
    "\n",
    "<b>Policy:</b> Final working strategy the Agent identifies. A function that takes the current environment state to return an action.\n",
    "$\\pi(s): S -> R$\n",
    "\n",
    "<b>Episode:</b> Represents one run of the game. The Agent learns across many epizodes. # of episodes used to train is a hyperparameter. \n",
    "\n",
    "<b>Terminal State:</b> End state after an episode.\n",
    "\n",
    "Where $t$ is time-step t.\n",
    "\n",
    "An action durring a state results in a reward for that action and a new updated state.\n",
    "$S(t), A(t) -> R(t+1),S(t+1)$\n",
    "This is also represented by a 4-tuple with the notation $(s, a, r, s')$.\n",
    "\n",
    "\n",
    "\n",
    "<b>Value Function:</b> $V(S)$ the value of a state that considers future rewards as well as immediate rewards. It is the value of a state that takes into account the probability of all possible future rewards. \n",
    "Each state will have a value that is based on all possible future rewards.  Tells you how valueable being in a state is by taking into consideration possible future rewards. \n",
    "$V(S) = E(all future rewards | S(t)=s)$\n",
    "\n",
    "<b>Attribution</b> How to attribute rewards to actions such that long term rewards are considered. \n",
    "\n",
    "<b>Total Discounted Reward</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class TicTacToe():\n",
    "    def __init__(self):\n",
    "        self.p1_state = []\n",
    "        self.p2_state = []\n",
    "        self.past_actions = []\n",
    "        self.drawboard()\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        check_game = self.game_over()\n",
    "        while not check_game:\n",
    "            self.update_board(player_turn=\"p1\")\n",
    "            check_game = self.game_over()\n",
    "            if not check_game:\n",
    "                self.update_board(player_turn=\"p2\")\n",
    "                check_game = self.game_over()\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    def value_function():\n",
    "        return\n",
    "    \n",
    "    def reward():\n",
    "        return\n",
    "    \n",
    "    def drawboard(self):\n",
    "        try:\n",
    "            for i in range(len(self.p1_state)):\n",
    "                plt.plot(self.p1_state[i][0], self.p1_state[i][1], 'ro', color=\"red\")\n",
    "        except:\n",
    "            plt.plot([], 'ro', color=\"red\")\n",
    "        try:\n",
    "            for i in range(len(self.p2_state)):\n",
    "                plt.plot(self.p2_state[i][0], self.p2_state[i][1], 'v', color=\"blue\")\n",
    "        except:\n",
    "             plt.plot([], 'v', color=\"blue\")\n",
    "                \n",
    "        plt.xlim([0, 6])\n",
    "        plt.ylim([0, 6])\n",
    "        plt.axvline(x=2, color=\"black\")\n",
    "        plt.axvline(x=4, color=\"black\")\n",
    "        plt.axhline(y=2, color=\"black\")\n",
    "        plt.axhline(y=4, color=\"black\")\n",
    "\n",
    "    \n",
    "    def update_state(self, state):\n",
    "        \n",
    "        #select action\n",
    "        new_x, new_y = self.get_next_coordinates()\n",
    "        while ((new_x, new_y) in self.past_actions):\n",
    "            new_x, new_y = self.get_next_coordinates()\n",
    "        \n",
    "        #update state with action\n",
    "        else:\n",
    "            state.append((new_x,new_y))\n",
    "            self.past_actions.append((new_x,new_y))\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next_coordinates():\n",
    "        new_x = np.random.choice([1, 3, 5], 1)[0]\n",
    "        new_y = np.random.choice([1, 3, 5], 1)[0]\n",
    "        return new_x, new_y\n",
    "\n",
    "\n",
    "    def update_board(self, player_turn):\n",
    "        \n",
    "        if player_turn == \"p1\":\n",
    "            self.p1_state = self.update_state(self.p1_state)\n",
    "        else:\n",
    "            self.p2_state = self.update_state(self.p2_state) \n",
    "\n",
    "        self.drawboard()\n",
    "        display(plt)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "    def game_over(self):\n",
    "    \n",
    "        #diagonal wins\n",
    "        diagonal_win_a = [(1, 5), (3, 3), (5, 1)]\n",
    "        if (set(diagonal_win_a) <= set(self.p1_state)):\n",
    "            print(\"Player 1 wins!\")\n",
    "            return True\n",
    "        if (set(diagonal_win_a) <= set(self.p2_state)):\n",
    "            print(\"Player 2 wins!\")\n",
    "            return True\n",
    "        diagonal_win_b = [(5, 1), (3, 3), (1, 5)]\n",
    "        if (set(diagonal_win_b) <= set(self.p1_state)):\n",
    "            print(\"Player 1 wins!\")\n",
    "            return True\n",
    "        if (set(diagonal_win_b) <= set(self.p2_state)):\n",
    "            print(\"Player 2 wins!\")\n",
    "            return True\n",
    "            \n",
    "        \n",
    "        #vertical wins\n",
    "        vertical_win_a = [(1,1), (1,3), (1,5)]\n",
    "        vertical_win_b = [(3,1), (3,3), (3,5)]\n",
    "        vertical_win_c = [(5,1), (5,3), (5,5)]\n",
    "        if (set(vertical_win_a) <= set(self.p1_state)):\n",
    "            print(\"Player 1 wins!\")\n",
    "            return True\n",
    "        if (set(vertical_win_a) <= set(self.p2_state)):\n",
    "            print(\"Player 2 wins!\")\n",
    "            return True\n",
    "        if (set(vertical_win_b) <= set(self.p1_state)):\n",
    "            print(\"Player 1 wins!\")\n",
    "            return True\n",
    "        if (set(vertical_win_b) <= set(self.p2_state)):\n",
    "            print(\"Player 2 wins!\")\n",
    "            return True\n",
    "        if (set(vertical_win_c) <= set(self.p1_state)):\n",
    "            print(\"Player 1 wins!\")\n",
    "            return True\n",
    "        if (set(vertical_win_c) <= set(self.p2_state)):\n",
    "            print(\"Player 2 wins!\")\n",
    "            return True\n",
    "\n",
    "    \n",
    "        #horizontal wins\n",
    "        horizontal_win_a = [(1,1), (3,1), (5,1)]\n",
    "        horizontal_win_b = [(1,3), (3,3), (5,3)]\n",
    "        horizontal_win_c = [(1,5), (3,5), (5,5)]\n",
    "        if (set(horizontal_win_a) <= set(self.p1_state)):\n",
    "            print(\"Player 1 wins!\")\n",
    "            return True\n",
    "        if (set(horizontal_win_a) <= set(self.p2_state)):\n",
    "            print(\"Player 2 wins!\")\n",
    "            return True\n",
    "        if (set(horizontal_win_b) <= set(self.p1_state)):\n",
    "            print(\"Player 1 wins!\")\n",
    "            return True\n",
    "        if (set(horizontal_win_b) <= set(self.p2_state)):\n",
    "            print(\"Player 2 wins!\")\n",
    "            return True\n",
    "        if (set(horizontal_win_c) <= set(self.p1_state)):\n",
    "            print(\"Player 1 wins!\")\n",
    "            return True\n",
    "        if (set(horizontal_win_c) <= set(self.p2_state)):\n",
    "            print(\"Player 2 wins!\")\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        if len(self.past_actions) >= 9:\n",
    "            print(\"Stalemate!\")\n",
    "            return True\n",
    "        \n",
    "        else: \n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 wins!\n"
     ]
    }
   ],
   "source": [
    "TicTacToe().play()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cute_Robot():\n",
    "    '''Agent class that plays TicTacToeEpisodes to learn a policy. \n",
    "    Uses Epsilon Greedy.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "          def __init__(self, eps=0.1, alpha=0.5):\n",
    "    self.eps = eps # probability of choosing random action instead of greedy\n",
    "    self.alpha = alpha # learning rate\n",
    "    self.verbose = False\n",
    "    self.state_history = []\n",
    "  \n",
    "    def setV(self, V):\n",
    "        self.V = V\n",
    "\n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "\n",
    "    def set_verbose(self, v):\n",
    "      # if true, will print values for each position on the board\n",
    "      self.verbose = v\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.state_history = []\n",
    "    \n",
    "    #one episode\n",
    "    TicTacToe().play()  \n",
    "    \n",
    "    #TODO\n",
    "    #Add value function\n",
    "    #Add reward (update valiue function?)\n",
    "    #explore function (Randomly select spot)\n",
    "    #Exploit function (Use value function or reward to select spot?)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make two Agents play against eachother. \n",
    "#Instantiated Agent class will have policy (value function?) as an attribute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender_engines",
   "language": "python",
   "name": "recommender_engines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
